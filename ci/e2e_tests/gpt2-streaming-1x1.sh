#!/bin/bash
set -e

TRAINING_NAME=gpt2-stream-1x1-e2e-$TRAINING_SUFFIX
# We use -D llama-tokenized-oag as a dummy dataset as, currently, if no input dataset is provided, no output will be generated by FCS.
flexai training run $TRAINING_NAME -u $SOURCE -b $TRAINING_REVISION -D ci-llama-tokenized-oag -- code/causal-language-modeling/train.py \
    --do_train \
    --eval_strategy no \
    --dataset_name HuggingFaceFW/fineweb \
    --dataset_config_name CC-MAIN-2024-10 \
    --dataset_streaming true \
    --dataset_group_text true \
    --dataloader_num_workers 8 \
    --max_steps 99 \
    --model_name_or_path openai-community/gpt2 \
    --output_dir /output \
    --per_device_train_batch_size 8 \
    --logging_steps 10 \
    --save_steps 50

./ci/wait_for_training.sh $TRAINING_NAME
timeout 300 flexai training logs $TRAINING_NAME > logs.txt || { echo "Error: Timeout while getting logs."; exit 1; }
echo "Checking log content..."
grep "Streaming dataset" logs.txt
if grep -q "Loading tokenized dataset from:" logs.txt; then
  echo "Error: logs contain the string 'Loading tokenized dataset from:'"
  exit 1
fi
grep "Total train batch size (w. parallel, distributed & accumulation) = 8" logs.txt
echo "Checking fetch content..."
flexai training fetch $TRAINING_NAME
unzip -l output_0.zip | grep output/checkpoint-50/model.safetensors
unzip -l output_0.zip | grep output/checkpoint-99/model.safetensors
unzip -l output_0.zip | grep output/model.safetensors
